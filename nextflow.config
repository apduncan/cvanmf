// SHARED PARAMETERS
// The matrix to be decomposed
params.matrix = "resources/test_data.tsv"
// Number of random shuffles of matrix
params.num_shuffles = 100
// Max NMF iterations, shared across all tasks
params.max_iter = 3000
// Random seed
params.seed = 4298
// Initialisation method
params.init = "random" 
// Beta-loss function
params.beta_loss = "kullback-leibler"

// RANK SELECTION PARAMETERS
// Lower and upper bound of ranks to search during rank selection, inclusive
params.ranks = "2,20"

// REGULARISATION PARAMETERS
// Regularisation values to search
params.alpha = [0, 0.03125, 0.0625, 0.125, 0.25, 0.5, 1, 2, 5]
// Scale regularisations based on number of samples in input
// The way sklearn applies regularisation changed from previous versions,
// alpha is now multiplied by number of samples. To get the same behaviour
// as the ES paper, but with more recent versions of sklearn, we divide the
// alpha by the number of samples. This gives approximately the same behaviour
// for the range of alphas used for default selection in ES paper.
params.scale_alpha = true
// Regularisation ratio
// This is the ratio of sparsifying (L1) to densifying (L2) regularisation to
// be applied. Defaults to L1. This is used in rank selection, but with an
// alpha value of 0.0, meaning no regularisation is applied during rank
// selection
params.l1_ratio = 1.0
// Ranks to search in regularisation
// Regularisation is not done by default, as it a lot more iterations and is
// only needed for ranks you are interested in. After runnning rank selection,
// you can add some ranks to be checked in here - they do not need to 
// be sequential 
params.regu_rank = []

// FULL DECOMPOSITION PARAMETERS
// Number of randomly initialised decompositions on full matrix
// when getting decompositions for a given rank. Applied for both regularised
// and non-regularised model.
params.random_starts = 100
// Whether to calculate stability based rank selection methods (copehentic
// correlation, dispersion, signature similarity). This significantly increases
// execution time and memory requirements, and our benchmarks found them no 
// better than bicrossvalidation for rank selection. Stability based
// coefficients will never be calculated for regularised models.
params.stability = true

// OUTPUT PARAMS
params.publish_dir = "output"


profiles {
    conda {
        conda.enabled = true
        conda.useMamba = true
        process.conda = "${projectDir}/env.yaml"
    }
    slurm {
        // This is by default set up for our cluster configuration for typical gut metagenome taxonomic data
        // Conda parameters
        conda.useMicromamba = true
        // SLURM parameters
        process {
            conda = "${projectDir}/env.yaml"
            executor = 'slurm'
            queue = 'ei-short'
            memory = '1.5GB'
            cpus = 4
            withName: "rankBiCv|reguBiCv" {
                // Processes which typically launch a lot of small jobs should
                // submitted as job arrays on SLURM to help the scheduler
                array: 500
            },
            withLabel: largemem {
                // Typically our processes requiring a lot of memory are
                // splitting and merging data, and not very computationally
                // demanding
                queue = 'ei-short'
                memory = '8GB'
                cpus = 2
            }
            withLabel: largememandcpu {
                // Processes which need high mem, and will utilise multiple cpu
                // This is typically the full matrix decompositions
                queue = 'ei-short'
                memory = '8GB'
                cpus = 4
            }
        }
    }
    docker {
        // Profile for running locally, using a docker container for processes
        // You can run using containers on SLURM by specifying a process level
        // container or by providing at the command prompt i.e.
        // nextflow run apduncan/cvanmf -with-docker ghcr.io/apduncan/cvanmf:latest-nf
        // The latest image for the pipeline is always available at
        // ghcr.io/apduncan/cvanmf:latest-nf.
        // However, we put a specific version in this config by default so that
        // each github release runs the corresponding version container, to aid
        // reproducibility
        docker.enabled = true
        process.container = "ghcr.io/apduncan/cvanmf:0.3.1-nf"
    }
    singularity {
        // Profile for
    }
}

dag.verbose = true